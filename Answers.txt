Average return while learning:  -0.066238

Learned Policy:
 Usable Ace:
S S H S S S S S S S 20
H S S H H H S S S S 19
S H S H H S S S H H 18
S H H H H H H H H H 17
H H H H H H H H H H 16
H H H H H H H H H H 15
H H H H H H H H H H 14
H H H H H H H H H H 13
H H H H H H H H H H 12
1 2 3 4 5 6 7 8 9 10 

 No Usable Ace:
S S S S S S S S S S 20
S S S S S S S S S S 19
S S S S S S S S S S 18
S S S S S S S S S S 17
S S S S S S S S S S 16
S S S S S S S S H S 15
H S S S S S H H S H 14
H S S S S S H H H H 13
H S S S S S H S H H 12
1 2 3 4 5 6 7 8 9 10 

Average learned deterministic policy return:  -0.040728

3. First run the q learning algorithm and stop the learning

On runs of 1 000 000, with epsilon and alpha step sizes of 0.1:
	best alpha = 0.1
	best epsilon = 0.4
	average return of -0.050578

On runs of 10 000, with epsilon and alpha step sizes of 0.01:
	best alpha = 0.07, 0.01
	best epsilon = 0.76, 0.79
	average return of -.0207, -0.0227
	
	
increment 0.01, 0.01, 10000
Return:  -0.0224
Epsilon:  0.8200000000000005
Alpha:  0.02

increment 0.01, 0.001, 20000
Return:  -0.01915
Epsilon:  0.853
Alpha:  0.01

alpha = 0.015
increment epsilon by 0.0005
50000 episodes per epsilon value
Return:  -0.0305
Epsilon:  0.3130

Alpha:  0.01
increment epsilon by 0.0005
50000 episodes per epsilon value
Return:  -0.02738
Epsilon:  0.3185


Run size 1000000, alpha=0.01, epsilon=0.3
 Usable Ace:
H S S S S S S S S S 20
S S S S H S S H S H 19
S S S S H S S S S S 18
H H H H S S S H H S 17
H H S H H H H H H H 16
S S H H H H H H S H 15
H H H H S H H H H H 14
H H S H H S H H H H 13
H H S S S H H H H H 12
1 2 3 4 5 6 7 8 9 10

  No Usable Ace:
S S S S S S S S S S 20
S S S S S S S S S S 19
S S S S S S S S S S 18
S S S S S S S S S S 17
S S S S S S H S S S 16
S S S S S S S H S S 15
H S S S S S H S S H 14
H H S S S S H H H H 13
H H S S S S H H H H 12
1 2 3 4 5 6 7 8 9 10
Average learned policy return: -0.045001